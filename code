import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv("adult.csv")
df.head()
df.isnull().sum()
df.columns=['Age','Workclass','Fnlwgt','Education','education_num','marital_status','occupation' ,'relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','income']
df.head()
df.duplicated()
df.dtypes
print(df['Workclass'].value_counts())
print(df['marital_status'].value_counts())
print(df['occupation'].value_counts())
print(df['relationship'].value_counts())
print(df['race'].value_counts())
print(df['Education'].value_counts())
print(df['sex'].value_counts())
print(df['native_country'].value_counts())
print(df['income'].value_counts())
from sklearn.preprocessing import LabelEncoder
lb = LabelEncoder()
df['Workclass'] = lb.fit_transform(df['Workclass'])
df['relationship'] = lb.fit_transform(df['relationship'])
df['occupation'] = lb.fit_transform(df['occupation'])
df['marital_status'] = lb.fit_transform(df['marital_status'])
df['race'] = lb.fit_transform(df['race'])
df['Education'] = lb.fit_transform(df['Education'])
df['sex'] = lb.fit_transform(df['sex'])
df['native_country'] = lb.fit_transform(df['native_country'])
df['income'] = lb.fit_transform(df['income'])
df.dtypes
x = df.iloc[:,:-1]
y = df.iloc[:,-1]
print(type(x))
print(type(y))
print(x.shape)
print(y.shape)
x.head(32561)
y.head(32561)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix,classification_report
def apply_model(model,x_train,x_test,y_train,y_test):
    model.fit(x_train,y_train)
    ypred = model.predict(x_test)
    print('Predictions\n',ypred)
    print('Training Score',model.score(x_train,y_train))
    print('Testing Score',model.score(x_test,y_test))
    cm = confusion_matrix(y_test,ypred)
    print('Confusion_matrix\n',cm)
    print('Classification_report\n',classification_report(y_test,ypred))
m1 = DecisionTreeClassifier(criterion='entropy',max_depth=14,min_samples_split=150)
apply_model(m1,x_train,x_test,y_train,y_test)
mrate1 = float((1 - accuracy_score(y_test,m1.predict(x_test))))
print('mislassification rate for decision tree classifier',mrate1)
m2 = RandomForestClassifier(n_estimators=80,criterion='gini',max_depth=9,min_samples_split=12)
apply_model(m2,x_train,x_test,y_train,y_test)
mrate2 = float((1 - accuracy_score(y_test,m2.predict(x_test))))
print('mislassification rate for random forest classifier',mrate2)
m3 = KNeighborsClassifier(n_neighbors=19)
apply_model(m3,x_train,x_test,y_train,y_test)
mrate3 = float((1 - accuracy_score(y_test,m3.predict(x_test))))
print('mislassification rate for KNeighbours classifier',mrate3)
m4 = LogisticRegression(solver='liblinear')
apply_model(m4,x_train,x_test,y_train,y_test)
mrate4 = float((1 - accuracy_score(y_test,m4.predict(x_test))))
print('mislassification rate for LogisticRegression classifier',mrate4)
m5 = SVC(kernel='linear',C=1)
apply_model(m5,x_train,x_test,y_train,y_test)
mrate5 = float((1 - accuracy_score(y_test,m5.predict(x_test))))
print('mislassification rate for SVM classifier',mrate5)




